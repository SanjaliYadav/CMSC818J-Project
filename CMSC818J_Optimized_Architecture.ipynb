{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set up for Google Colab "
      ],
      "metadata": {
        "id": "-nMehcKtM4Rs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zjMLPgLaOxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8224df-0aa3-44cb-acc5-4a55c3f6a1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "from scipy.io import mmread\n",
        "from scipy.io import mminfo\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "datAddr = \"/content/drive/MyDrive/CMSC818J_Test/\"\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import csv\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimized Architecture \n",
        " "
      ],
      "metadata": {
        "id": "XquetqYhjW2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assumptions\n",
        "1. Each multiplication and accumulation takes 3 cycles\n",
        "2. Reading the rows from memory takes 2 cycles\n",
        "3. Storing the result of multiplication and accumulation takes 1 cycle. NO merging required since we are implementing inner product. \n",
        "4. PEs write back the output in parallel and can read the input in parallel\n",
        "5. We will sort 8 rows at a time as our assumption is that given our bandwidth, we can read 8 rows at a time"
      ],
      "metadata": {
        "id": "y7bc1CwVDcqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENUMS for PE_stats\n",
        "BUSY_CYCLES = 1 \n",
        "MAX_NUM_NON_ZEROS = 2\n",
        "MIN_NUM_NON_ZEROS = 3\n",
        "IDLE_CYCLES = 4\n",
        "FIFO_BUFFER_LENGTH = 8\n",
        "\n",
        "CYCLES_TO_READ = 8\n",
        "CYCLES_TO_SORT = 6\n",
        "CYCLES_TO_PUT_BUFFER = 1 # per element\n",
        "CYCLES_TO_MUL_AND_ACCUMULATE = 6 # per element multiplication\n",
        "WRITE_BACK_RATE = 1000 # can write 1000 elements at a time "
      ],
      "metadata": {
        "id": "5it-wVriX3Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adds the same number of idle cycles to each of PEs\n",
        "def add_idle_cycles (cycles_num):\n",
        "  for i in range(total_PEs):\n",
        "    PE_stats[i][IDLE_CYCLES] += cycles_num "
      ],
      "metadata": {
        "id": "_P-LemEKz6g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adds the different number of idle cycles to each of PEs by subtracting the \n",
        "# max_non_per_PE which represents the cycles for the slowest PEs from the busy \n",
        "# cycles of the other PEs once after all the allocations are finished. \n",
        "def add_idle_cycles2(idle_cycle_list, max_nonzero_per_PE, PE_stats):\n",
        "  for i in range(total_PEs):\n",
        "    PE_stats[i][IDLE_CYCLES] += max_nonzero_per_PE - idle_cycle_list[i] \n",
        "  return PE_stats"
      ],
      "metadata": {
        "id": "ODhtnKR-bvof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks whether the busy and idle cycles add up to the total number of \n",
        "# cycles for each of the output files.  \n",
        "def check_cycle_accuracy(filename):\n",
        "  for pe in PE_stats:\n",
        "    if pe[BUSY_CYCLES] + pe[IDLE_CYCLES] != TOTAL_NUMBER_OF_CYCLES:\n",
        "      print(filename + \" Doesn't match up\" + str(total_PEs))"
      ],
      "metadata": {
        "id": "_idk3ebsV_ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulates PE MAC operations\n",
        "def PE_mult(output_col_num, PE_num):\n",
        "  total_cycles_spent = 0 \n",
        "  PE_to_process = PE_lst.pop(0)\n",
        "  row_num = PE_output_index.pop(0) \n",
        "  accumulation = 0\n",
        "  # optimization for inner product: whichever dictionary has smallest non-zeros, we iterate over that. \n",
        "  if len(B_dict) < len(PE_to_process):\n",
        "    for col,data in B_dict.items():\n",
        "      if col in PE_to_process:\n",
        "        # multiply and then accumulate  \n",
        "        accumulation += PE_to_process[col] * data \n",
        "        PE_stats[PE_num][BUSY_CYCLES] += CYCLES_TO_MUL_AND_ACCUMULATE\n",
        "        total_cycles_spent += 1 \n",
        "    output_mat2[row_num][output_col_num] = accumulation \n",
        "  else:\n",
        "    for col,data in PE_to_process.items():\n",
        "      if col in B_dict:\n",
        "        # multiply and then accumulate  \n",
        "        accumulation += B_dict[col] * data \n",
        "        PE_stats[PE_num][BUSY_CYCLES] += CYCLES_TO_MUL_AND_ACCUMULATE\n",
        "        total_cycles_spent += 1 \n",
        "    output_mat2[row_num][output_col_num] = accumulation \n",
        "  # Calculate max number of cycles  \n",
        "  if len(PE_to_process) > PE_stats[PE_num][MAX_NUM_NON_ZEROS]: \n",
        "      PE_stats[PE_num][MAX_NUM_NON_ZEROS] = len(PE_to_process)\n",
        "  elif len(PE_to_process) < PE_stats[PE_num][MIN_NUM_NON_ZEROS]: \n",
        "      PE_stats[PE_num][MIN_NUM_NON_ZEROS] = len(PE_to_process)\n",
        "    \n",
        "  return total_cycles_spent   \n",
        "\n"
      ],
      "metadata": {
        "id": "2KF7JUygy4YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Read the rows from matrices and add them to FIFO buffer\n",
        "def sort_computer_rows(data_index_A, col_index_A, pointer_index_A, output_row_index):\n",
        "  PE_rows_read = dict() \n",
        "  PE_rows_index = dict()\n",
        "  max_nonzero_per_PE = 0\n",
        "  cycles_to_sort = 0\n",
        "  idle_cycle_list = [] # keeps track of idle cycles for each PE per allocation\n",
        "  # Because of FIFO buffer length, there are lower number of cycles\n",
        "  for i in range(FIFO_BUFFER_LENGTH):\n",
        "    if pointer_index_A < mat_csrA_len:\n",
        "      numOfElems = (mat_csr.indptr[pointer_index_A + 1] - mat_csr.indptr[pointer_index_A])\n",
        "      data_row_A = mat_csr.data[data_index_A:numOfElems+data_index_A]\n",
        "      row_A = mat_csr.indices[col_index_A:numOfElems+data_index_A]\n",
        "      if numOfElems in PE_rows_read: \n",
        "        PE_rows_read[numOfElems].append(dict(zip(row_A, data_row_A)))\n",
        "        PE_rows_index[numOfElems].append(output_row_index)\n",
        "        data_index_A += numOfElems\n",
        "        col_index_A += numOfElems\n",
        "        pointer_index_A += 1\n",
        "        output_row_index += 1\n",
        "      else:\n",
        "        PE_rows_read[numOfElems] = []\n",
        "        PE_rows_read[numOfElems].append(dict(zip(row_A, data_row_A)))\n",
        "        PE_rows_index[numOfElems] = []\n",
        "        PE_rows_index[numOfElems].append(output_row_index)\n",
        "        data_index_A += numOfElems\n",
        "        col_index_A += numOfElems\n",
        "        pointer_index_A += 1\n",
        "        output_row_index += 1 \n",
        "    else:\n",
        "      break\n",
        "\n",
        "  # sort the rows in the buffer and then add then to the FIFO which is PE_lst\n",
        "  non_zero_keys = list(PE_rows_read.keys())\n",
        "  non_zero_keys.sort(reverse = True)\n",
        "  cycles_to_sort += CYCLES_TO_SORT\n",
        "  \n",
        "  for k in non_zero_keys:\n",
        "    for PE_rows in PE_rows_read[k]:\n",
        "      PE_lst.append(PE_rows)\n",
        "      PE_output_index.append(PE_rows_index[k].pop(0)) # keeps track of row num after sorting\n",
        "    cycles_to_sort += CYCLES_TO_PUT_BUFFER\n",
        "  \n",
        "  return (data_index_A, col_index_A, pointer_index_A, output_row_index, cycles_to_sort)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2K3KyTO2D4-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the matrices and multiply them by each other \n",
        "directory = os.fsencode(datAddr)\n",
        "for pe_num in range(2, 10, 2):\n",
        "  with open('/content/drive/MyDrive/CMSC818J_Data/Optimized_PE' + str(pe_num) + '_16fifo.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)   \n",
        "    for file in os.listdir(directory):\n",
        "      filename = os.fsdecode(file)\n",
        "      print(filename)\n",
        "\n",
        "      ## Read the file \n",
        "      m = mmread(datAddr+filename)\n",
        "      mat_csr = m.tocsr() # convert the first matrix to csr \n",
        "      mat_csrB = m.tocsc() # convert the first matrix to csc\n",
        "\n",
        "      total_PEs= pe_num # number of PEs available. Can change this number as required\n",
        "      PE_stats = [] # Keeps track of the metrics\n",
        "\n",
        "      for i in range(total_PEs):\n",
        "        PE_stat = dict()\n",
        "        PE_stat[BUSY_CYCLES] = 0 \n",
        "        PE_stat[MAX_NUM_NON_ZEROS] = 0 \n",
        "        PE_stat[MIN_NUM_NON_ZEROS ] = sys.maxsize\n",
        "        PE_stat[IDLE_CYCLES] = 0 \n",
        "        PE_stats.append(PE_stat)\n",
        "        PE_stats\n",
        "      \n",
        "\n",
        "      max_total_cycles = 0\n",
        "      mat_csrB_len = len(mat_csrB.indptr) - 1\n",
        "      mat_csrA_len = len(mat_csr.indptr) - 1\n",
        "      output_mat_len = len(mat_csrB.indptr) - 1\n",
        "      PE_num = 0\n",
        "      # acts as the buffer that stores rows to be allocated to PEs\n",
        "      PE_lst = []\n",
        "      PE_output_index = []\n",
        "      # matrix to store the file output \n",
        "      output_mat2 = [[0 for _ in range(output_mat_len)] for _ in range(output_mat_len)]\n",
        "      data_index_B = 0 \n",
        "      row_index_B = 0\n",
        "      pointer_index_B = 0 \n",
        "      output_col_index = 0\n",
        "      output_row_index = 0\n",
        "      cycles_to_sort = 0\n",
        "\n",
        "      # Total number of cycles is whichever PE took the longest \n",
        "      TOTAL_NUMBER_OF_CYCLES = 0 \n",
        "\n",
        "      TOTAL_NUMBER_OF_CYCLES += CYCLES_TO_READ # For reading elements, one time cost rest of them are amortized with PE computations\n",
        "      add_idle_cycles(CYCLES_TO_READ)\n",
        "\n",
        "\n",
        "      for j in range(mat_csrB_len): \n",
        "        # Decode col of matrix B from csc format\n",
        "        numOfElems_B = (mat_csrB.indptr[pointer_index_B + 1] - mat_csrB.indptr[pointer_index_B])\n",
        "        data_row_B = mat_csrB.data[data_index_B:numOfElems_B+data_index_B]\n",
        "        col_B = mat_csrB.indices[row_index_B:numOfElems_B+data_index_B]\n",
        "\n",
        "        # The decoded cols and corresponding data values are zipped into a dictionary \n",
        "        # data structure to easily pattern match with rows when computing inner product\n",
        "        B_dict = dict(zip(col_B, data_row_B))\n",
        "        data_index_B += numOfElems_B\n",
        "        row_index_B += numOfElems_B\n",
        "        pointer_index_B += 1\n",
        "        data_index_A = 0 # these values need to updated in if not PE\n",
        "        col_index_A = 0\n",
        "        pointer_index_A = 0\n",
        "        output_row_index = 0\n",
        "        for i in range(mat_csrA_len):\n",
        "          # Decode rows of matrix A from csr format \n",
        "          if not PE_lst:\n",
        "            data_index_A, col_index_A, pointer_index_A, output_row_index, cycles_to_sort = sort_computer_rows(data_index_A, col_index_A, pointer_index_A, output_row_index)\n",
        "            # allocate it to a PE using round-robin strategy \n",
        "          total_computation_cycles = PE_mult(output_col_index, PE_num)\n",
        "          PE_stats[PE_num][IDLE_CYCLES] += math.ceil(total_computation_cycles/WRITE_BACK_RATE)\n",
        "          PE_num = (PE_num + 1) % total_PEs \n",
        "        output_col_index += 1 \n",
        "\n",
        "      # Keeps track of max total number of cycles and once all the allocations are \n",
        "      # finished the idle cycles are added. The PEs that had the longest workload \n",
        "      # is used for calculating idle cycles for other PEs.  \n",
        "      max_cycles = []\n",
        "      for pe in range(pe_num):\n",
        "        max_cycles.append(PE_stats[pe][BUSY_CYCLES])\n",
        "        max_total_cycles = max(PE_stats[pe][BUSY_CYCLES] + PE_stats[pe][IDLE_CYCLES], max_total_cycles)\n",
        "      TOTAL_NUMBER_OF_CYCLES += max_total_cycles\n",
        "      max_num = max(max_cycles)\n",
        "\n",
        "      string = [filename, TOTAL_NUMBER_OF_CYCLES]\n",
        "      for pe in range(pe_num):\n",
        "        for stat_num in range(1, 5, 1):\n",
        "          if stat_num == IDLE_CYCLES:\n",
        "            # assumption is that the rest of the PEs will wait for the main PE\n",
        "            # The add_idle_cycles2 computes the number of cycles \n",
        "            # each PE has to wait for the slowest PE to finish computation. \n",
        "            idle_cycles_to_add = max_num - PE_stats[pe][BUSY_CYCLES]\n",
        "            string.append(PE_stats[pe][stat_num] + idle_cycles_to_add)\n",
        "          else:\n",
        "            string.append(PE_stats[pe][stat_num])\n",
        "      output_mat2_reshape = np.reshape(output_mat2, (output_mat_len, output_mat_len))\n",
        "      correct_output = np.dot(m.todense(),m.todense())\n",
        "      # Validates if matrix multiplication is done correctly\n",
        "      print(np.allclose(output_mat2_reshape, correct_output))\n",
        "      # write back to the output file \n",
        "      writer.writerow(string)\n",
        "\n"
      ],
      "metadata": {
        "id": "6mQTeT1Pf6Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the matrices and multiply them by a randomly generated matrix\n",
        "directory = os.fsencode(datAddr)\n",
        "\n",
        "for pe_num in range(2, 10, 2):\n",
        "  with open('/content/drive/MyDrive/CMSC818J_Data/Optimized_PEAB' + str(pe_num) + '_16fifo.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)   \n",
        "    for file in os.listdir(directory):\n",
        "      filename = os.fsdecode(file)\n",
        "      print(filename)\n",
        "\n",
        "      ## Read the file \n",
        "      m = mmread(datAddr+filename)\n",
        "      m2 = mmread(\"/content/drive/MyDrive/psmigr_3.mtx\") # serves as the random matrix \n",
        "      mat_csr = m.tocsr() # convert the first matrix to csr \n",
        "      mat_csrB = m2.tocsc() # convert the second matrix to csc \n",
        "      mat_csrB.resize(mat_csr.shape) # reshape so that it can be multiplied to first matrix\n",
        "\n",
        "      total_PEs= pe_num # number of PEs available. Can change this number as required\n",
        "      PE_stats = [] # to store the metrics\n",
        "\n",
        "      for i in range(total_PEs):\n",
        "        PE_stat = dict()\n",
        "        PE_stat[BUSY_CYCLES] = 0 \n",
        "        PE_stat[MAX_NUM_NON_ZEROS] = 0 \n",
        "        PE_stat[MIN_NUM_NON_ZEROS ] = sys.maxsize\n",
        "        PE_stat[IDLE_CYCLES] = 0 \n",
        "        PE_stats.append(PE_stat)\n",
        "        PE_stats\n",
        "      \n",
        "\n",
        "      max_total_cycles = 0\n",
        "      mat_csrB_len = len(mat_csrB.indptr) - 1\n",
        "      mat_csrA_len = len(mat_csr.indptr) - 1\n",
        "      # output matrix size \n",
        "      output_mat_lenX = len(mat_csr.indptr) - 1\n",
        "      output_mat_lenY = len(mat_csrB.indptr) - 1\n",
        "      PE_num = 0\n",
        "      # acts as the buffer that stores rows to be allocated to PEs\n",
        "      PE_lst = []\n",
        "      PE_output_index = []\n",
        "      # matrix to store the file output \n",
        "      output_mat2 = np.zeros((output_mat_lenX, output_mat_lenY))\n",
        "      data_index_B = 0 \n",
        "      row_index_B = 0\n",
        "      pointer_index_B = 0 \n",
        "      output_col_index = 0\n",
        "      output_row_index = 0\n",
        "      cycles_to_sort = 0\n",
        "\n",
        "      # Total number of cycles is whichever PE took the longest \n",
        "      TOTAL_NUMBER_OF_CYCLES = 0 \n",
        "\n",
        "      TOTAL_NUMBER_OF_CYCLES += CYCLES_TO_READ # For reading elements, one time cost rest of them are amortized with PE computations\n",
        "      add_idle_cycles(CYCLES_TO_READ)\n",
        "\n",
        "\n",
        "      for j in range(mat_csrB_len): \n",
        "        # Decode col of matrix B from csc format\n",
        "        numOfElems_B = (mat_csrB.indptr[pointer_index_B + 1] - mat_csrB.indptr[pointer_index_B])\n",
        "        data_row_B = mat_csrB.data[data_index_B:numOfElems_B+data_index_B]\n",
        "        col_B = mat_csrB.indices[row_index_B:numOfElems_B+data_index_B]\n",
        "\n",
        "        # The decoded cols and corresponding data values are zipped into a dictionary \n",
        "        # data structure to easily pattern match with rows when computing inner product\n",
        "        B_dict = dict(zip(col_B, data_row_B))\n",
        "        data_index_B += numOfElems_B\n",
        "        row_index_B += numOfElems_B\n",
        "        pointer_index_B += 1\n",
        "        data_index_A = 0 # these values need to updated in if not PE\n",
        "        col_index_A = 0\n",
        "        pointer_index_A = 0\n",
        "        output_row_index = 0\n",
        "        for i in range(mat_csrA_len):\n",
        "          # Decode rows of matrix A from csr format \n",
        "          if not PE_lst:\n",
        "            data_index_A, col_index_A, pointer_index_A, output_row_index, cycles_to_sort = sort_computer_rows(data_index_A, col_index_A, pointer_index_A, output_row_index)\n",
        "            # allocate it to a PE using round-robin strategy \n",
        "          total_computation_cycles = PE_mult(output_col_index, PE_num)\n",
        "          PE_stats[PE_num][IDLE_CYCLES] += math.ceil(total_computation_cycles/WRITE_BACK_RATE)\n",
        "          PE_num = (PE_num + 1) % total_PEs \n",
        "        output_col_index += 1 \n",
        "      \n",
        "      # Keeps track of max total number of cycles and once all the allocations are \n",
        "      # finished the idle cycles are added. The PEs that had the longest workload \n",
        "      # is used for calculating idle cycles for other PEs.  \n",
        "      max_cycles = []\n",
        "      for pe in range(pe_num):\n",
        "        max_cycles.append(PE_stats[pe][BUSY_CYCLES])\n",
        "        max_total_cycles = max(PE_stats[pe][BUSY_CYCLES] + PE_stats[pe][IDLE_CYCLES], max_total_cycles)\n",
        "      TOTAL_NUMBER_OF_CYCLES += max_total_cycles\n",
        "      max_num = max(max_cycles)\n",
        "\n",
        "      string = [filename, TOTAL_NUMBER_OF_CYCLES]\n",
        "      for pe in range(pe_num):\n",
        "        for stat_num in range(1, 5, 1):\n",
        "          if stat_num == IDLE_CYCLES:\n",
        "            # assumption is that the rest of the PEs will wait for the main PE\n",
        "            # The add_idle_cycles2 computes the number of cycles \n",
        "            # each PE has to wait for the slowest PE to finish computation\n",
        "            idle_cycles_to_add = max_num - PE_stats[pe][BUSY_CYCLES]\n",
        "            string.append(PE_stats[pe][stat_num] + idle_cycles_to_add)\n",
        "          else:\n",
        "            string.append(PE_stats[pe][stat_num])\n",
        "      output_mat2_reshape = np.reshape(output_mat2, (output_mat_lenX, output_mat_lenY))\n",
        "      correct_output = (mat_csr * mat_csrB).todense()   \n",
        "      # Validates if matrix multiplication is done correctly\n",
        "      print(np.allclose(output_mat2_reshape, correct_output))\n",
        "      writer.writerow(string)"
      ],
      "metadata": {
        "id": "PhXBrdLx_gGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}